#Exercise 1

import pandas as pd

titanic = pd.read_csv('titanic.csv')
print("Initial Data:\n", titanic.head())

missing_values = titanic.isnull().sum()
print("\nMissing Values:\n", missing_values)

titanic['Age_mean_imputed'] = titanic['Age'].fillna(titanic['Age'].mean())
titanic['Age_median_imputed'] = titanic['Age'].fillna(titanic['Age'].median())
titanic['Embarked_mode_imputed'] = titanic['Embarked'].fillna(titanic['Embarked'].mode()[0])

titanic_dropped_rows = titanic.dropna(subset=['Cabin'])
titanic_dropped_column = titanic.drop(columns=['Cabin'])

print("\nOriginal missing values:\n", missing_values)
print("\nAfter mean imputation (Age):\n", titanic['Age_mean_imputed'].isnull().sum())
print("After median imputation (Age):\n", titanic['Age_median_imputed'].isnull().sum())
print("After mode imputation (Embarked):\n", titanic['Embarked_mode_imputed'].isnull().sum())
print("After dropping rows with missing 'Cabin':\n", titanic_dropped_rows.isnull().sum())
print("After dropping the 'Cabin' column:\n", titanic_dropped_column.isnull().sum())



#Exercise 2

import pandas as pd
from sklearn.preprocessing import LabelEncoder

car_eval = pd.read_csv('car_evaluation.csv')

print("Initial Data:\n", car_eval.head())
print("\nData Types:\n", car_eval.dtypes)

car_eval_one_hot = pd.get_dummies(car_eval)
car_eval_label_encoded = car_eval.apply(LabelEncoder().fit_transform)

print("\nOriginal dataset shape:", car_eval.shape)
print("One-hot encoded dataset shape:", car_eval_one_hot.shape)
print("Label encoded dataset shape:", car_eval_label_encoded.shape)

print("\nOriginal Dataset:\n", car_eval.head())
print("\nOne-Hot Encoded Dataset:\n", car_eval_one_hot.head())
print("\nLabel Encoded Dataset:\n", car_eval_label_encoded.head())



#Exercise 3

import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

wine_data = pd.read_csv('winequality-red.csv', sep=';')

scaler = MinMaxScaler()
wine_data_scaled = scaler.fit_transform(wine_data)
wine_data_scaled = pd.DataFrame(wine_data_scaled, columns=wine_data.columns)

print("Normalized Data:")
print(wine_data_scaled.head())
print(wine_data_scaled.describe())

scaler = StandardScaler()
wine_data_standardized = scaler.fit_transform(wine_data)
wine_data_standardized = pd.DataFrame(wine_data_standardized, columns=wine_data.columns)

print("\nStandardized Data:")
print(wine_data_standardized.head())
print(wine_data_standardized.describe())



#Exercise 4

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

df = pd.read_csv('HousingData.csv')

z_scores = np.abs(stats.zscore(df))
outliers_z = df[(z_scores > 3).any(axis=1)]

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
outliers_iqr = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.title("Boxplot of Boston Housing Dataset")
plt.show()

df_no_outliers = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

print("Outliers using Z-score:")
print(outliers_z)
print("\nOutliers using IQR:")
print(outliers_iqr)
print("\nDataset after removing outliers:")
print(df_no_outliers.head())



#Exercise 5

import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

df = pd.read_csv('retail_sales_dataset.csv')

numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
non_numerical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns

knn_imputer = KNNImputer(n_neighbors=5)
df_numerical = df[numerical_cols]
df_knn_imputed_numerical = pd.DataFrame(knn_imputer.fit_transform(df_numerical), columns=numerical_cols)

df_knn_imputed = pd.concat([df_knn_imputed_numerical, df[non_numerical_cols]], axis=1)

print("\nMissing values after KNN imputation:")
print(df_knn_imputed.isnull().sum())

mice_imputer = IterativeImputer(max_iter=10, random_state=0)
df_mice_imputed_numerical = pd.DataFrame(mice_imputer.fit_transform(df_numerical), columns=numerical_cols)
df_mice_imputed = pd.concat([df_mice_imputed_numerical, df[non_numerical_cols]], axis=1)

print("\nMissing values after MICE imputation:")
print(df_mice_imputed.isnull().sum())



#Exercise 6

import pandas as pd

df = pd.read_csv('heart_disease_data.csv')

df['age_group'] = pd.cut(df['age'], bins=[20, 40, 60, 80], labels=['young', 'adult', 'enior'])
df['cholesterol_level'] = pd.cut(df['chol'], bins=[150, 200, 250, 300], labels=['normal', 'borderline', 'high'])
df['bp_category'] = pd.cut(df['trestbps'], bins=[90, 120, 140, 160], labels=['normal', 'prehypertension', 'tage1'])
df['risk_factors'] = (df['fbs'] > 0).astype(int) + (df['exang'] > 0).astype(int) + (df['oldpeak'] > 0).astype(int)
df['avg_max_hr'] = df.groupby('sex')['thalach'].transform('mean')

print(df.head())


#Exercise 7

import pandas as pd
import numpy as np
from scipy import stats

df = pd.read_csv('day.csv')

skewed_vars = ['temp', 'atemp', 'hum', 'windspeed']
for var in skewed_vars:
    print(f"{var}: {stats.skew(df[var])}")

df['log_temp'] = np.log(df['temp'])
df['log_atemp'] = np.log(df['atemp'])
df['log_hum'] = np.log(df['hum'] + 1e-6)
df['log_windspeed'] = np.log(df['windspeed'])

df['sqrt_temp'] = np.sqrt(df['temp'])
df['sqrt_atemp'] = np.sqrt(df['atemp'])
df['sqrt_hum'] = np.sqrt(df['hum'] + 1e-6)
df['sqrt_windspeed'] = np.sqrt(df['windspeed'])

df['bc_temp'], _ = stats.boxcox(df['temp'])
df['bc_atemp'], _ = stats.boxcox(df['atemp'])
df['bc_hum'], _ = stats.boxcox(df['hum'] + 1e-6)
df['bc_windspeed'], _ = stats.boxcox(df['windspeed'])

print(df.head())


#Exercise 8

import pandas as pd
from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('diabetes.csv')

corr_matrix = df.corr()
top_corr_features = corr_matrix['Outcome'].abs().sort_values(ascending=False).head(6).index.tolist()
print("Top correlated features:", top_corr_features)

X = df.drop('Outcome', axis=1)
y = df['Outcome']
mutual_info = mutual_info_classif(X, y)
top_mutual_info_features = X.columns[np.argsort(mutual_info)][-5:].tolist()
print("Top mutual information features:", top_mutual_info_features)

log_reg = LogisticRegression()
rfe = RFE(log_reg, n_features_to_select=5)
rfe.fit(X, y)
rfe_features = X.columns[rfe.support_].tolist()
print("RFE features:", rfe_features)

selector = SelectKBest(mutual_info_classif, k=5)
selector.fit(X, y)
select_kbest_features = X.columns[selector.get_support()].tolist()
print("SelectKBest features:", select_kbest_features)



#Exercise 9

import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN
from imblearn.under_sampling import RandomUnderSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

df = pd.read_csv('creditcard.csv')

print(df.shape)
print(df['Class'].value_counts())

smote = SMOTE(random_state=42)
X_res_smote, y_res_smote = smote.fit_resample(df.drop('Class', axis=1), df['Class'])
print("SMOTE:")
print(y_res_smote.value_counts())

adasyn = ADASYN(random_state=42)
X_res_adasyn, y_res_adasyn = adasyn.fit_resample(df.drop('Class', axis=1), df['Class'])
print("\nADASYN:")
print(y_res_adasyn.value_counts())

rus = RandomUnderSampler(random_state=42)
X_res_rus, y_res_rus = rus.fit_resample(df.drop('Class', axis=1), df['Class'])
print("\nUndersampling:")
print(y_res_rus.value_counts())

rf = RandomForestClassifier(random_state=42)
rf.fit(df.drop('Class', axis=1), df['Class'])
y_pred = rf.predict(df.drop('Class', axis=1))
print("\nOriginal dataset:")
print("Accuracy:", accuracy_score(df['Class'], y_pred))
print("Classification Report:\n", classification_report(df['Class'], y_pred))

for X_res, y_res, name in [(X_res_smote, y_res_smote, "SMOTE"), (X_res_adasyn, y_res_adasyn, "ADASYN"), (X_res_rus, y_res_rus, "Undersampling")]:
    rf = RandomForestClassifier(random_state=42)
    rf.fit(X_res, y_res)
    y_pred = rf.predict(X_res)
    print("\nBalanced dataset ({}):".format(name))
    print("Accuracy:", accuracy_score(y_res, y_pred))
    print("Classification Report:\n", classification_report(y_res, y_pred))



#Exercise 10
